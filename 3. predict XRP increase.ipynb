{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MySQLdb as mdb\n",
    "import sys\n",
    "import sqlalchemy as sql\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from   sklearn import preprocessing \n",
    "from   sklearn.decomposition import PCA\n",
    "from   sklearn.feature_selection import SelectKBest\n",
    "from   sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect directly to MySQL DB*/\n",
    "conn= mdb.connect(host='xxxxxx',user='xxxx',password='xxxxxxxxxxxx',db='mcm_practicum')\n",
    "# xrp\n",
    "xrp= pd.read_sql(\n",
    "'SELECT t1.coin , t1.close_price ,t1.close_timestamp as close_time'\n",
    "', tim as tweet_time , num_tweets , fav_percent , retweet_percent , avg_senti , min_senti , max_senti '\n",
    "'FROM mcm_practicum.xrpbtc as t1 '\n",
    "'INNER JOIN ('\n",
    "'select tim, num_tweets , case when favs = 0 then 0 else favs/num_tweets end as fav_percent'\n",
    "', case when retweet = 0 then 0 else retweet/num_tweets end as retweet_percent'\n",
    "', retweet'\n",
    "',avg_senti'\n",
    "',max_senti'\n",
    "',min_senti '\n",
    "'from ( '\n",
    "'SELECT FROM_UNIXTIME(CEIL(UNIX_TIMESTAMP(tweet_date)/1800)*1800) as tim  /* rounding to nearest thirty minutes for aggregations */ '\n",
    "', count(distinct concat(id,tweet_date)) as num_tweets'\n",
    "', sum(favorites) as favs'\n",
    "', sum(retweets)  as retweet'\n",
    "', avg(sentiment) as avg_senti'\n",
    "', MIN(sentiment) min_senti'\n",
    "', MAX(sentiment) max_senti '\n",
    "'FROM mcm_practicum.xrp_tweets '\n",
    "'GROUP BY tim)st1 '\n",
    "')t2 '\n",
    "'on t2.tim = date_sub(t1.close_timestamp, interval 30 minute) /* stagger data so sentiment occurred at least thirty minutes before price */'\n",
    "';', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp = xrp.sort_values(by=['close_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp = xrp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(xrp.close_time,xrp.close_price , color = '#e74c3c', label='Ripple', linewidth = 0.5)\n",
    "plt.title('Price in Bitcoin',fontsize = 9)\n",
    "plt.xlabel('Date',fontsize = 9)\n",
    "plt.legend(loc='best',fontsize = 9)\n",
    "plt.xticks(fontsize = 8)\n",
    "plt.yticks(fontsize = 8)\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(xrp.close_time,xrp.close_price , color = '#e74c3c', linewidth = 1.5)\n",
    "plt.title('Ripple Price',fontsize = 14)\n",
    "plt.xlabel('Date',fontsize = 13)\n",
    "plt.ylabel('Price in Bitcoin',fontsize = 13)\n",
    "plt.xticks(fontsize = 11)\n",
    "plt.yticks(fontsize = 11)\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the disparity in number of records\n",
    "print(xrp.close_time.min()) \n",
    "print(xrp.close_time.max()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create log returns column\n",
    "xrp[\"close_log_return\"] = np.log(xrp.close_price / xrp.close_price.shift(1)) # log of (current price divided by previous price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create raw returns column\n",
    "xrp[\"close_raw_return\"] = (xrp.close_price / xrp.close_price.shift(1))-1 # raw return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrp.close_raw_return.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(xrp.close_time,xrp.close_raw_return , color = '#e74c3c')\n",
    "plt.legend(loc='best')\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create target\n",
    "conditions = [\n",
    "    (xrp['close_raw_return'] > 0) ,\n",
    "    (xrp['close_raw_return'] <= 0)]\n",
    "choices = [1, 0]\n",
    "xrp['target'] = np.select(conditions, choices, default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrp[['close_raw_return', 'target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrp.target.agg('sum') ,xrp.target.agg('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# refine dataframes\n",
    "xrp = xrp[['coin','target','close_time','close_raw_return','tweet_time', 'num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti', 'max_senti' , 'min_senti']]\n",
    "\n",
    "# drop NA (this the first row which can't calculate close_return as there is no previous close_price to calculate using)\n",
    "xrp = xrp.dropna()\n",
    "#reset the index\n",
    "xrp = xrp.reset_index(drop='True')\n",
    "\n",
    "#cast to float\n",
    "xrp.max_senti = xrp.max_senti.astype(float)\n",
    "xrp.min_senti = xrp.min_senti.astype(float)\n",
    "xrp.fav_percent = xrp.fav_percent.astype(float)\n",
    "xrp.retweet_percent = xrp.retweet_percent.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "xrp.plot.scatter(x='close_raw_return',y = 'avg_senti')\n",
    "plt.legend(loc='best')\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp = xrp[['coin','target', 'close_time', 'close_raw_return', 'tweet_time', 'num_tweets',\n",
    "       'fav_percent', 'retweet_percent', 'avg_senti', 'max_senti',\n",
    "       'min_senti']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrp.close_raw_return.head(10), xrp.close_raw_return.rolling(min_periods=4, window=4).mean().shift(1).head(8))\n",
    "#xrp.avg_senti.rolling(min_periods=6, window=6).mean().shift(1).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create additional variables\n",
    "# based on return\n",
    "xrp['avg_return_L2h'] = xrp.close_raw_return.rolling(min_periods=4, window=4).mean().shift(1) # avg returns for 2 hours before t where t is current time period\n",
    "xrp['avg_return_L6h'] = xrp.close_raw_return.rolling(min_periods=12, window=12).mean().shift(1) # avg returns for 6 hours before t where t is current time period\n",
    "xrp['avg_return_L12h'] = xrp.close_raw_return.rolling(min_periods=24, window=24).mean().shift(1) # avg returns for 12 hours before t where t is current time period\n",
    "\n",
    "# based on sentiment\n",
    "xrp['avg_senti_L2h'] = xrp.avg_senti.rolling(min_periods=4, window=4).mean().shift(1) # avg sentiment for 2 hours before t where t is current time period\n",
    "xrp['avg_senti_L6h'] = xrp.avg_senti.rolling(min_periods=12, window=12).mean().shift(-1) # avg sentiment for 6 hours before t where t is current time period\n",
    "xrp['avg_senti_L12h'] = xrp.avg_senti.rolling(min_periods=24, window=24).mean().shift(-1) # avg sentiment for 12 hours before t where t is current time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp[['close_time' , 'target', 'close_raw_return' ,'avg_return_L2h', 'avg_senti' , 'avg_senti_L2h']].head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prevalance in Data Set:\",xrp.target.sum() / xrp.target.count(),\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp[['coin', 'target', 'close_time', 'tweet_time',\n",
    "       'num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti',\n",
    "       'max_senti', 'min_senti', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_return_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train/ test split\n",
    "xrp_train = xrp.sample(frac=0.7,random_state=200)  \n",
    "xrp_test = xrp.drop(xrp_train.index)\n",
    "\n",
    "xrp_train = xrp_train[['target','num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti','max_senti', 'min_senti'\n",
    "  , 'avg_return_L2h', 'avg_return_L6h', 'avg_return_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h']]\n",
    "xrp_test = xrp_test[['target','num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti','max_senti', 'min_senti'\n",
    "  , 'avg_return_L2h', 'avg_return_L6h', 'avg_return_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prevalance in Training Set:\",xrp_train.target.sum() / xrp_train.target.count(),\"\")\n",
    "print(\"Prevalance in Testing Set:\",xrp_test.target.sum() / xrp_test.target.count(),\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of NaN in Training:\",xrp_train.isnull().sum().sum(),\", Number of NaN in Testing:\", xrp_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#records that are being dropped are ones that didn't have a sufficient number of records that occurred immediately prior \n",
    "#to them in order to create the derived variables i.e. avg_senti_L2h: the first four records in the dataset would have been dropped\n",
    "xrp_train = xrp_train.dropna()\n",
    "xrp_test = xrp_test.dropna()\n",
    "print(\"Number of NaN in Training:\",xrp_train.isnull().sum().sum(),\", Number of NaN in Testing:\", xrp_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the index for the training and test so that PCA sets can be merged back\n",
    "xrp_train = xrp_train.reset_index(drop=True)\n",
    "xrp_test = xrp_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp_train.num_tweets.quantile(.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#top decile\n",
    "quant90tr = []\n",
    "quant90tst = []\n",
    "\n",
    "for t in xrp_train.num_tweets:\n",
    "     if t > xrp_train.num_tweets.quantile(.90):\n",
    "            quant90tr.append(1)\n",
    "     else:\n",
    "            quant90tr.append(0)\n",
    "            \n",
    "\n",
    "for t in xrp_test.num_tweets:\n",
    "     if t > xrp_train.num_tweets.quantile(.90):#uses the training quartile to determine its value as test data is for validation only\n",
    "            quant90tst.append(1)\n",
    "     else:\n",
    "            quant90tst.append(0)            \n",
    "                \n",
    "xrp_train = pd.merge(xrp_train, pd.DataFrame(quant90tr), left_index=True, right_index=True)                             \n",
    "xrp_test = pd.merge(xrp_test, pd.DataFrame(quant90tst), left_index=True, right_index=True)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#top quartile\n",
    "quant75tr = []\n",
    "quant75tst = []\n",
    "\n",
    "for t in xrp_train.num_tweets:\n",
    "     if t > xrp_train.num_tweets.quantile(.75):\n",
    "            quant75tr.append(1)\n",
    "     else:\n",
    "            quant75tr.append(0)\n",
    "            \n",
    "\n",
    "for t in xrp_test.num_tweets:\n",
    "     if t > xrp_train.num_tweets.quantile(.75):#uses the training quartile to determine its value as test data is for validation only\n",
    "            quant75tst.append(1)\n",
    "     else:\n",
    "            quant75tst.append(0)            \n",
    "            \n",
    "xrp_train = pd.merge(xrp_train, pd.DataFrame(quant75tr), left_index=True, right_index=True)                             \n",
    "xrp_test = pd.merge(xrp_test, pd.DataFrame(quant75tst), left_index=True, right_index=True)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrp_train.columns = ['target', 'num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti',\n",
    "       'max_senti', 'min_senti', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_return_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h',\n",
    "       'top_decile_num_tweets', 'top_quartile_num_tweets']\n",
    "xrp_test.columns = ['target', 'num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti',\n",
    "       'max_senti', 'min_senti', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_return_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h',\n",
    "       'top_decile_num_tweets', 'top_quartile_num_tweets']\n",
    "print(xrp_train, xrp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('top Decile tweets in train:',xrp_train.top_decile_num_tweets.agg('sum') , 'top Decile tweets in test:',xrp_test.top_decile_num_tweets.agg('sum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler() #scaler\n",
    "\n",
    "#scaling\n",
    "xrp_train1 = xrp_train[xrp_train.columns.difference(['target'])]\n",
    "\n",
    "xrp_scaled_tr = pd.DataFrame(scaler.fit_transform(xrp_train1))\n",
    "xrp_scaled_tr.columns = xrp_train1.columns\n",
    "xrp_scaled_tr.index = xrp_train1.index\n",
    "print(xrp_scaled_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of how some of the variables are correlated in the fundamentals dataset\n",
    "sns.pairplot(xrp_scaled_tr[['num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti',\n",
    "       'max_senti', 'min_senti', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_return_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't include categorical cols or cols with ~25% NaN in imputation and PCA \n",
    "xrp_test1 = xrp_test[xrp_test.columns.difference(['target'])]\n",
    "\n",
    "xrp_scaled_tst = pd.DataFrame(scaler.transform(xrp_test1))\n",
    "xrp_scaled_tst.columns = xrp_test1.columns\n",
    "xrp_scaled_tst.index = xrp_test1.index\n",
    "xrp_scaled_tr= pd.merge(xrp_train[['target']], xrp_scaled_tr, left_index=True, right_index=True)\n",
    "print(xrp_scaled_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no negative values\n",
    "sum(n < 0 for n in xrp_scaled_tr[['num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti',\n",
    "       'max_senti', 'min_senti', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_return_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h',\n",
    "       'top_decile_num_tweets', 'top_quartile_num_tweets']].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univar_tr_target = xrp_train[['target']].values\n",
    "univar_tr_data = xrp_scaled_tr[['num_tweets', 'fav_percent', 'retweet_percent', 'avg_senti',\n",
    "       'max_senti', 'min_senti', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_return_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h',\n",
    "       'top_decile_num_tweets', 'top_quartile_num_tweets']].values\n",
    "univar = SelectKBest(score_func=chi2, k=4)\n",
    "fit_univar = univar.fit(univar_tr_data, univar_tr_target)\n",
    "# summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "print(fit_univar.scores_)\n",
    "univar_features = fit_univar.transform(univar_tr_data)\n",
    "# summarize selected features\n",
    "print(xrp_scaled_tr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in ('avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h','avg_senti', 'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h',\n",
    "'fav_percent', 'max_senti', 'min_senti', 'num_tweets','retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets'):\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.title('XRP %s'%i, fontsize = 9)\n",
    "    plt.xticks(fontsize = 8)\n",
    "    plt.yticks(fontsize = 8)\n",
    "    plt.hist(xrp_scaled_tr[i], normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = xrp_scaled_tr[['avg_return_L2h', 'avg_return_L6h', 'avg_return_L12h',\n",
    "       'avg_senti', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h',\n",
    "       'fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "       'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']].corr('spearman') #used Spearman as some vars not not normally distributed\n",
    "\n",
    "sns.set(font_scale=1.3)\n",
    "ax = plt.axes()\n",
    "sns.heatmap(corr,  \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "           cmap='Reds', ax=ax)\n",
    "ax.set_title('Ripple Correlation Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = xrp_scaled_tr[['avg_return_L2h', 'avg_return_L6h', 'avg_return_L12h',\n",
    "       'avg_senti', 'avg_senti_L2h', 'avg_senti_L6h', 'avg_senti_L12h',\n",
    "       'fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "       'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']].corr('spearman') #used Spearman as some vars not not normally distributed\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(2,2))\n",
    "sns.heatmap(corr,  \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "           cmap=\"Blues\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include Sentiment and Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_senti', 'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h',\n",
    "       'fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "       'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']])\n",
    "PCA_chk = pd.DataFrame(np.cumsum(pca.explained_variance_ratio_))\n",
    "PCA_chk['rule'] = PCA_chk.index+1\n",
    "PCA_chk.columns = ['cum_var' , 'PC']\n",
    "\n",
    "#graph\n",
    "sns.set_style('white')\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title('Ripple Scree Plot', fontsize = 14)\n",
    "plt.xticks(np.arange(min(PCA_chk['PC']), max(PCA_chk['PC'])+1, 1.0),fontsize = 13)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.plot(PCA_chk['PC'],PCA_chk['cum_var'], color = '#e74c3c', linewidth = 2.5)\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlabel('No. of Components',fontsize = 13)\n",
    "plt.ylabel('Cum. Explained Variance',fontsize = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# elbow appears to be at 5 Principle Components\n",
    "PCA_chk.to_csv('XRP PCA_rule_variant.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit PCA to Training & Apply to Training & Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 5) #account for 91% of variance\n",
    "pca.fit(xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_senti', 'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h',\n",
    "       'fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "       'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp_train1 = pd.DataFrame(pca.transform(xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_senti', 'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h',\n",
    "       'fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "       'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']]))\n",
    "xrp_test1 = pd.DataFrame(pca.transform(xrp_scaled_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_senti', 'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h',\n",
    "       'fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "       'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename PCA cols\n",
    "for c in xrp_train1.columns:\n",
    "    d = c+1                        #PCA and index + 1 as name\n",
    "    xrp_train1['PC%s'% d] = xrp_train1[c]\n",
    "\n",
    "#drop duplicate cols\n",
    "xrp_train1 = xrp_train1.filter(like='PC',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename PCA cols\n",
    "for c in xrp_test1.columns:\n",
    "    d = c+1                       #PCA and index + 1 as name\n",
    "    xrp_test1['PC%s'% d] = xrp_test1[c]\n",
    "\n",
    "#drop duplicate cols\n",
    "xrp_test1 = xrp_test1.filter(like='PC',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(xrp_train1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge PCA back to fundamentals\n",
    "xrp_trainPCA = pd.merge(xrp_train[['target']], xrp_train1, left_index=True, right_index=True)\n",
    "xrp_testPCA = pd.merge(xrp_test[['target']], xrp_test1, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclude Sentiment and Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']])\n",
    "PCA_chk = pd.DataFrame(np.cumsum(pca.explained_variance_ratio_))\n",
    "PCA_chk['rule'] = PCA_chk.index+1\n",
    "PCA_chk.columns = ['cum_var' , 'PC']\n",
    "\n",
    "#graph\n",
    "plt.plot(PCA_chk['PC'],PCA_chk['cum_var'], c= '#4286f4')\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# elbow appears to be at 2 Principle Components\n",
    "PCA_chk.to_csv('XRP PCA_rule_variant_no_tweet_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit PCA to Training & Apply to Training & Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2) #account for 90% of variance\n",
    "pca.fit(xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp_train_no_tweet_1 = pd.DataFrame(pca.transform(xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]))\n",
    "xrp_test_no_tweet_1 = pd.DataFrame(pca.transform(xrp_scaled_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename PCA cols\n",
    "for c in xrp_train_no_tweet_1.columns:\n",
    "    d = c+1                        #PCA and index + 1 as name\n",
    "    xrp_train_no_tweet_1['PC%s'% d] = xrp_train_no_tweet_1[c]\n",
    "\n",
    "#drop duplicate cols\n",
    "xrp_train_no_tweet_1 = xrp_train_no_tweet_1.filter(like='PC',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename PCA cols\n",
    "for c in xrp_test_no_tweet_1.columns:\n",
    "    d = c+1                       #PCA and index + 1 as name\n",
    "    xrp_test_no_tweet_1['PC%s'% d] = xrp_test_no_tweet_1[c]\n",
    "\n",
    "#drop duplicate cols\n",
    "xrp_test_no_tweet_1 = xrp_test_no_tweet_1.filter(like='PC',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(xrp_train_no_tweet_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge PCA back to fundamentals\n",
    "xrp_train_no_tweet_PCA = pd.merge(xrp_train[['target']], xrp_train_no_tweet_1, left_index=True, right_index=True)\n",
    "xrp_test_no_tweet_PCA = pd.merge(xrp_test[['target']], xrp_test_no_tweet_1, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import linear_model as lm \n",
    "from sklearn.tree import DecisionTreeClassifier as dt\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn import svm \n",
    "from sklearn import naive_bayes as nb\n",
    "from patsy import dmatrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrp_trainPCA)\n",
    "print(xrp_train_no_tweet_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrp_testPCA.isnull().sum().sum())\n",
    "print(xrp_test_no_tweet_PCA.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#records that are being dropped are ones that didn't have a sufficient number of records that occurred immediately prior \n",
    "#to them in order to create the derived variables i.e. avg_senti_L12h\n",
    "xrp_train = xrp_train.dropna()\n",
    "xrp_test = xrp_test.dropna()\n",
    "print(\"Number of NaN in Training:\",xrp_train.isnull().sum().sum(),\", Number of NaN in Testing:\", xrp_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xrp_trainPCA.columns)\n",
    "print(xrp_train_no_tweet_PCA.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cols = pd.DataFrame(columns = ['PC1', 'PC2', 'PC3', 'PC4' , 'PC5'])  #columns for modelling\n",
    "#convert train/ test to matrices for modelling\n",
    "xrp_data_train = xrp_trainPCA.as_matrix(columns=[ list(data_cols) ])\n",
    "xrp_target_train = xrp_trainPCA.as_matrix(columns=['target'])\n",
    "xrp_target_train = np.ravel(xrp_target_train)\n",
    "\n",
    "xrp_data_test = xrp_testPCA.as_matrix(columns=[ list(data_cols) ])\n",
    "xrp_target_test = xrp_testPCA.as_matrix(columns=['target'])\n",
    "xrp_target_test = np.ravel(xrp_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_cols_no_tweet = pd.DataFrame(columns = ['PC1', 'PC2'])  #columns for modelling\n",
    "#convert train/ test to matrices for modelling\n",
    "xrp_data_no_tweet_train = xrp_train_no_tweet_PCA.as_matrix(columns=[ list(data_cols_no_tweet) ])\n",
    "\n",
    "xrp_data_no_tweet_test = xrp_test_no_tweet_PCA.as_matrix(columns=[ list(data_cols_no_tweet) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(xrp_target_train).sum() / pd.DataFrame(xrp_target_train).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xrp_scaled_tr.columns\n",
    "xrp_scaled_tr.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweets\n",
    "xrp_scaled_tr = xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_senti', 'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h',\n",
    "       'fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "       'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']]\n",
    "\n",
    "xrp_scaled_tr = xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h',\n",
    "       'avg_senti', 'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h',\n",
    "       'fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "       'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']]\n",
    "\n",
    "xrp_trainPCA = xrp_trainPCA[['PC1' , 'PC2', 'PC3' ,'PC4', 'PC5']]\n",
    "xrp_testPCA = xrp_testPCA[['PC1' , 'PC2', 'PC3' ,'PC4', 'PC5']]\n",
    "\n",
    "\n",
    "#no tweets\n",
    "xrp_scaled_no_tweet_tr = xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]\n",
    "\n",
    "xrp_scaled_no_tweet_tst = xrp_scaled_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]\n",
    "\n",
    "xrp_train_no_tweet_PCA = xrp_train_no_tweet_PCA[['PC1' , 'PC2']]\n",
    "xrp_test_no_tweet_PCA = xrp_test_no_tweet_PCA[['PC1' , 'PC2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter & Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression: Parameter and Feature Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_tr ,xrp_trainPCA):\n",
    "     for solv in ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'):\n",
    "            for c in (.001, 0.1, 1, 10, 100):\n",
    "                #Recursive Feature Elimination\n",
    "                model_RFE_lr = lm.LogisticRegression(C = c,solver = solv,class_weight = 'balanced')\n",
    "                # create the RFECV model\n",
    "                rfe_lr = RFECV(model_RFE_lr, step=1, scoring='roc_auc')   #'roc_auc' \n",
    "                rfe_lr = rfe_lr.fit(X, xrp_target_train)\n",
    "                result_lr = rfe_lr.support_\n",
    "                rank_lr = rfe_lr.ranking_\n",
    "                RFE_cols_lr = pd.DataFrame(result_lr, columns = ['Keep'])\n",
    "                RFE_rank_lr = pd.DataFrame(rank_lr, columns = ['Rank'])\n",
    "                data_cols_df = pd.DataFrame(X.columns.tolist(), columns =['Variable'])\n",
    "                RFE_cols_lr['indexs'] = RFE_cols_lr.index\n",
    "                RFE_rank_lr['indexs'] = RFE_rank_lr.index\n",
    "                data_cols_df['indexs'] = data_cols_df.index\n",
    "                keep_vars_lr = pd.merge(data_cols_df, RFE_cols_lr , on=['indexs'])\n",
    "                keep_vars_lr = pd.merge(keep_vars_lr, RFE_rank_lr , on=['indexs'])\n",
    "                keep_vars_lr = keep_vars_lr.drop('indexs', axis=1)\n",
    "                matr=confusion_matrix(xrp_target_train, rfe_lr.predict(X)).ravel()\n",
    "                print(\"C=\",c,\"Algorithm=\",solv, \"ROC:\" , metrics.roc_auc_score(xrp_target_train, rfe_lr.predict_proba(X)[:, 1]), keep_vars_lr[:25],metrics.classification_report(xrp_target_train, rfe_lr.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_no_tweet_tr ,xrp_train_no_tweet_PCA):\n",
    "     for solv in ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'):\n",
    "            for c in (.001, 0.1, 1, 10, 100):\n",
    "                #Recursive Feature Elimination\n",
    "                model_RFE_lr = lm.LogisticRegression(C = c,solver = solv,class_weight = 'balanced')\n",
    "                # create the RFECV model\n",
    "                rfe_lr = RFECV(model_RFE_lr, step=1, scoring='roc_auc')   #'roc_auc' \n",
    "                rfe_lr = rfe_lr.fit(X, xrp_target_train)\n",
    "                result_lr = rfe_lr.support_\n",
    "                rank_lr = rfe_lr.ranking_\n",
    "                RFE_cols_lr = pd.DataFrame(result_lr, columns = ['Keep'])\n",
    "                RFE_rank_lr = pd.DataFrame(rank_lr, columns = ['Rank'])\n",
    "                data_cols_df = pd.DataFrame(X.columns.tolist(), columns =['Variable'])\n",
    "                RFE_cols_lr['indexs'] = RFE_cols_lr.index\n",
    "                RFE_rank_lr['indexs'] = RFE_rank_lr.index\n",
    "                data_cols_df['indexs'] = data_cols_df.index\n",
    "                keep_vars_lr = pd.merge(data_cols_df, RFE_cols_lr , on=['indexs'])\n",
    "                keep_vars_lr = pd.merge(keep_vars_lr, RFE_rank_lr , on=['indexs'])\n",
    "                keep_vars_lr = keep_vars_lr.drop('indexs', axis=1)\n",
    "                matr=confusion_matrix(xrp_target_train, rfe_lr.predict(X)).ravel()\n",
    "                print(\"C=\",c,\"Algorithm=\",solv, \"ROC:\" , metrics.roc_auc_score(xrp_target_train, rfe_lr.predict_proba(X)[:, 1]), keep_vars_lr[:25],metrics.classification_report(xrp_target_train, rfe_lr.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Training & Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaled raw data\n",
    "xrp_data_lr_tr2 = xrp_scaled_tr[['avg_return_L12h', 'avg_return_L6h', 'avg_senti_L12h', 'min_senti', 'num_tweets'\n",
    "                                 ,'top_decile_num_tweets', 'top_quartile_num_tweets']]\n",
    "xrp_data_lr_tst2 = xrp_scaled_tst[['avg_return_L12h', 'avg_return_L6h', 'avg_senti_L12h', 'min_senti', 'num_tweets'\n",
    "                                 ,'top_decile_num_tweets', 'top_quartile_num_tweets']]\n",
    "    \n",
    "#PCA data\n",
    "xrp_data_lr_tr3 = xrp_trainPCA[['PC1','PC2', 'PC3', 'PC4', 'PC5']]\n",
    "xrp_data_lr_tst3 = xrp_testPCA[['PC1','PC2', 'PC3', 'PC4', 'PC5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaled raw data\n",
    "xrp_data_lr_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L12h']]\n",
    "xrp_data_lr_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L12h']]\n",
    "\n",
    "#PCA data\n",
    "xrp_data_lr_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1','PC2']]\n",
    "xrp_data_lr_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1','PC2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Parameter and Feature Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_tr, xrp_trainPCA ):\n",
    "    for c in ('gini' , 'entropy'):\n",
    "            for depth in (1,5,10,15,20):\n",
    "                for split in (.1,.2,.3,.4,.5):\n",
    "                    for leaf in (.1,.2,.3):\n",
    "                        #Recursive Feature Elimination\n",
    "                        model_RFE_dt = dt(criterion = c,class_weight = 'balanced'\n",
    "                                           , max_depth=depth, min_samples_split=split\n",
    "                                           , min_samples_leaf=leaf)\n",
    "                        # create the RFECV model\n",
    "                        rfe_dt = RFECV(model_RFE_dt, step=1, scoring='roc_auc')   #'roc_auc' \n",
    "                        rfe_dt = rfe_dt.fit(X, xrp_target_train)\n",
    "                        result_dt = rfe_dt.support_\n",
    "                        rank_dt = rfe_dt.ranking_\n",
    "                        RFE_cols_dt = pd.DataFrame(result_dt, columns = ['Keep'])\n",
    "                        RFE_rank_dt = pd.DataFrame(rank_dt, columns = ['Rank'])\n",
    "                        data_cols_df = pd.DataFrame(X.columns.tolist(), columns =['Variable'])\n",
    "                        RFE_cols_dt['indexs'] = RFE_cols_dt.index\n",
    "                        RFE_rank_dt['indexs'] = RFE_rank_dt.index\n",
    "                        data_cols_df['indexs'] = data_cols_df.index\n",
    "                        keep_vars_dt = pd.merge(data_cols_df, RFE_cols_dt , on=['indexs'])\n",
    "                        keep_vars_dt = pd.merge(keep_vars_dt, RFE_rank_dt , on=['indexs'])\n",
    "                        keep_vars_dt = keep_vars_dt.drop('indexs', axis=1)\n",
    "                        matr=confusion_matrix(xrp_target_train, rfe_dt.predict(X)).ravel()\n",
    "                        print(\"Criterion=\",c, \"Max Depth=\",depth,\"Min Samples Split=\",split,\"Min Samples Leaf=\",leaf, \"ROC:\" , metrics.roc_auc_score(xrp_target_train, rfe_dt.predict_proba(X)[:, 1]), keep_vars_dt[:25],metrics.classification_report(xrp_target_train, rfe_dt.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_no_tweet_tr ,xrp_train_no_tweet_PCA):\n",
    "    for c in ('gini' , 'entropy'):\n",
    "            for depth in (1,5,10,15,20):\n",
    "                for split in (.1,.2,.3,.4,.5):\n",
    "                    model_RFE_dt = dt(criterion = c,class_weight = 'balanced'\n",
    "                                       , max_depth=depth, min_samples_split=split\n",
    "                                       , min_samples_leaf=0.1)\n",
    "                    # create the RFECV model\n",
    "                    rfe_dt = RFECV(model_RFE_dt, step=1, scoring='roc_auc')   #'roc_auc' \n",
    "                    rfe_dt = rfe_dt.fit(X, xrp_target_train)\n",
    "                    result_dt = rfe_dt.support_\n",
    "                    rank_dt = rfe_dt.ranking_\n",
    "                    RFE_cols_dt = pd.DataFrame(result_dt, columns = ['Keep'])\n",
    "                    RFE_rank_dt = pd.DataFrame(rank_dt, columns = ['Rank'])\n",
    "                    data_cols_df = pd.DataFrame(X.columns.tolist(), columns =['Variable'])\n",
    "                    RFE_cols_dt['indexs'] = RFE_cols_dt.index\n",
    "                    RFE_rank_dt['indexs'] = RFE_rank_dt.index\n",
    "                    data_cols_df['indexs'] = data_cols_df.index\n",
    "                    keep_vars_dt = pd.merge(data_cols_df, RFE_cols_dt , on=['indexs'])\n",
    "                    keep_vars_dt = pd.merge(keep_vars_dt, RFE_rank_dt , on=['indexs'])\n",
    "                    keep_vars_dt = keep_vars_dt.drop('indexs', axis=1)\n",
    "                    matr=confusion_matrix(xrp_target_train, rfe_dt.predict(X)).ravel()\n",
    "                    print(\"Criterion=\",c, \"Max Depth=\",depth,\"Min Samples Split=\",split, \"ROC:\" , metrics.roc_auc_score(xrp_target_train, rfe_dt.predict_proba(X)[:, 1]), keep_vars_dt[:25],metrics.classification_report(xrp_target_train, rfe_dt.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Training & Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaled raw data\n",
    "xrp_data_dt_tr2 = xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h', 'avg_senti'\n",
    "                                 , 'avg_senti_L2h', 'avg_senti_L6h', 'fav_percent','max_senti', 'min_senti', 'num_tweets'\n",
    "                                 , 'retweet_percent','top_decile_num_tweets', 'top_quartile_num_tweets']]\n",
    "xrp_data_dt_tst2 = xrp_scaled_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h', 'avg_senti'\n",
    "                                 , 'avg_senti_L2h', 'avg_senti_L6h', 'fav_percent','max_senti', 'min_senti', 'num_tweets'\n",
    "                                 , 'retweet_percent','top_decile_num_tweets', 'top_quartile_num_tweets']]\n",
    "#PCA data\n",
    "xrp_data_dt_tr3 = xrp_trainPCA[['PC2','PC4']]\n",
    "xrp_data_dt_tst3 = xrp_testPCA[['PC2','PC4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaled raw data\n",
    "xrp_data_dt_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]\n",
    "xrp_data_dt_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]\n",
    "\n",
    "#PCA data\n",
    "xrp_data_dt_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1']]\n",
    "xrp_data_dt_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM: Parameter and Feature Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_tr,xrp_trainPCA, xrp_scaled_tr[['avg_senti_L12h','min_senti','top_decile_num_tweets']]):\n",
    "    for kern in ('rbf','linear', 'poly', 'sigmoid'):\n",
    "        for c in (.001, 0.1, 1, 10, 100):\n",
    "            for g in (0.001, 0.01, 0.1, 1):\n",
    "                #All variables\n",
    "                rfe_svm = svm.SVC(probability = True,kernel = kern,C= c,class_weight = 'balanced', gamma = g)\n",
    "                rfe_svm = rfe_svm.fit(X, xrp_target_train)                \n",
    "                matr=confusion_matrix(xrp_target_train, rfe_svm.predict(X)).ravel()\n",
    "                print(\"Data=\",X.iloc[0,1:3],\"C=\",c, \"Kernel=\",kern,\"Gamma=\",g,\"ROC=\",metrics.roc_auc_score(xrp_target_train, rfe_svm.predict_proba(X)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_no_tweet_tr ,xrp_train_no_tweet_PCA):\n",
    "    for kern in ('rbf','linear', 'poly', 'sigmoid'):\n",
    "        for c in (.001, 0.1, 1, 10, 100):\n",
    "            for g in (0.001, 0.01, 0.1, 1):\n",
    "                #All variables\n",
    "                rfe_svm = svm.SVC(probability = True,kernel = kern,C= c,class_weight = 'balanced', gamma = g)\n",
    "                rfe_svm = rfe_svm.fit(X, xrp_target_train)                \n",
    "                matr=confusion_matrix(xrp_target_train, rfe_svm.predict(X)).ravel()\n",
    "                print(\"Data=\",X.iloc[0,1:3],\"C=\",c, \"Kernel=\",kern,\"Gamma=\",g,\"ROC=\",metrics.roc_auc_score(xrp_target_train, rfe_svm.predict_proba(X)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best features for SVM According to RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaled raw data\n",
    "xrp_data_svm_tr2 = xrp_scaled_tr\n",
    "xrp_data_svm_tst2 = xrp_scaled_tst\n",
    "\n",
    "#PCA data\n",
    "xrp_data_svm_tr3 = xrp_trainPCA[['PC1','PC2','PC3','PC4','PC5']].as_matrix()\n",
    "xrp_data_svm_tst3 = xrp_testPCA[['PC1','PC2','PC3','PC4','PC5']].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaled raw data\n",
    "xrp_data_svm_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]\n",
    "xrp_data_svm_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]\n",
    "\n",
    "#PCA data\n",
    "xrp_data_svm_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1','PC2']]\n",
    "xrp_data_svm_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1','PC2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes: Parameter and Feature Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_tr ,xrp_trainPCA):\n",
    "    model_RFE_nb =  nb.GaussianNB()\n",
    "    # RFECV doesn't work\n",
    "    rfe_nb = model_RFE_nb.fit(X, xrp_target_train)\n",
    "    print(\"Data=\",X.iloc[0,1:3],\"ROC:\" , metrics.roc_auc_score(xrp_target_train, rfe_nb.predict_proba(X)[:, 1]),metrics.classification_report(xrp_target_train, rfe_nb.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_no_tweet_tr ,xrp_train_no_tweet_PCA):\n",
    "    model_RFE_nb =  nb.GaussianNB()\n",
    "    # RFECV doesn't work\n",
    "    rfe_nb = model_RFE_nb.fit(X, xrp_target_train)\n",
    "    print(\"Data=\",X.iloc[0,1:3],\"ROC:\" , metrics.roc_auc_score(xrp_target_train, rfe_nb.predict_proba(X)[:, 1]),metrics.classification_report(xrp_target_train, rfe_nb.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best features for Naive Bayes According to RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA data\n",
    "xrp_data_nb_tr3 = xrp_trainPCA[['PC1','PC2','PC3','PC4','PC5']]\n",
    "xrp_data_nb_tst3 = xrp_testPCA[['PC1','PC2','PC3','PC4','PC5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA data\n",
    "xrp_data_nb_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1','PC2']]\n",
    "xrp_data_nb_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1','PC2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Parameter and Feature Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_tr ,xrp_trainPCA):\n",
    "    for b in (True, False):\n",
    "        for split in (0.1, 0.2 ,0.3):\n",
    "            for n_est in (100, 300, 500):\n",
    "                model_RFE_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = b, max_depth = 5, \n",
    "                                   min_samples_leaf = 0.1, min_samples_split = split\n",
    "                                  , n_estimators = n_est)\n",
    "                # create the RFECV model\n",
    "                rfe_rf = RFECV(model_RFE_rf, step=1,  scoring='roc_auc')   #'roc_auc' \n",
    "                rfe_rf = rfe_rf.fit(X, xrp_target_train)\n",
    "                result_rf = rfe_rf.support_\n",
    "                rank_rf = rfe_rf.ranking_\n",
    "                RFE_cols_rf = pd.DataFrame(result_rf, columns = ['Keep'])\n",
    "                RFE_rank_rf = pd.DataFrame(rank_rf, columns = ['Rank'])\n",
    "                data_cols_df = pd.DataFrame(X.columns.tolist(), columns =['Variable'])\n",
    "                RFE_cols_rf['indexs'] = RFE_cols_rf.index\n",
    "                RFE_rank_rf['indexs'] = RFE_rank_rf.index\n",
    "                data_cols_df['indexs'] = data_cols_df.index\n",
    "                keep_vars_rf = pd.merge(data_cols_df, RFE_cols_rf , on=['indexs'])\n",
    "                keep_vars_rf = pd.merge(keep_vars_rf, RFE_rank_rf , on=['indexs'])\n",
    "                keep_vars_rf = keep_vars_rf.drop('indexs', axis=1)\n",
    "                matr=confusion_matrix(xrp_target_train, rfe_rf.predict(X)).ravel()\n",
    "                print(\"Bootstrap=\",b,\"Max Depth=\",5,\"Min Samples Split=\",split\n",
    "                      ,\"No. of Estimators\",n_est,\"ROC:\" , metrics.roc_auc_score(xrp_target_train, rfe_rf.predict_proba(X)[:, 1])\n",
    "                      ,keep_vars_rf[:25],metrics.classification_report(xrp_target_train, rfe_rf.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (xrp_scaled_no_tweet_tr ,xrp_train_no_tweet_PCA):\n",
    "    for b in (True, False):\n",
    "        for split in (0.1, 0.2 ,0.3):\n",
    "                for n_est in (100, 300, 500):\n",
    "                    model_RFE_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = b, max_depth = 5, \n",
    "                                      min_samples_leaf = 0.1, min_samples_split = split\n",
    "                                      , n_estimators = n_est)\n",
    "                    # create the RFECV model\n",
    "                    rfe_rf = RFECV(model_RFE_rf, step=1,  scoring='roc_auc')   #'roc_auc' \n",
    "                    rfe_rf = rfe_rf.fit(X, xrp_target_train)\n",
    "                    result_rf = rfe_rf.support_\n",
    "                    rank_rf = rfe_rf.ranking_\n",
    "                    RFE_cols_rf = pd.DataFrame(result_rf, columns = ['Keep'])\n",
    "                    RFE_rank_rf = pd.DataFrame(rank_rf, columns = ['Rank'])\n",
    "                    data_cols_df = pd.DataFrame(X.columns.tolist(), columns =['Variable'])\n",
    "                    RFE_cols_rf['indexs'] = RFE_cols_rf.index\n",
    "                    RFE_rank_rf['indexs'] = RFE_rank_rf.index\n",
    "                    data_cols_df['indexs'] = data_cols_df.index\n",
    "                    keep_vars_rf = pd.merge(data_cols_df, RFE_cols_rf , on=['indexs'])\n",
    "                    keep_vars_rf = pd.merge(keep_vars_rf, RFE_rank_rf , on=['indexs'])\n",
    "                    keep_vars_rf = keep_vars_rf.drop('indexs', axis=1)\n",
    "                    matr=confusion_matrix(xrp_target_train, rfe_rf.predict(X)).ravel()\n",
    "                    print(\"Bootstrap=\",b,\"Max Depth=\",5,\"Min Samples Split=\",split\n",
    "                          ,\"No. of Estimators\",n_est,\"ROC:\" , metrics.roc_auc_score(xrp_target_train, rfe_rf.predict_proba(X)[:, 1])\n",
    "                          ,keep_vars_rf[:25],metrics.classification_report(xrp_target_train, rfe_rf.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best features for Random Forest According to RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaled raw data\n",
    "xrp_data_rf_tr2 = xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h','avg_senti', 'avg_senti_L12h'\n",
    "                                 , 'avg_senti_L2h', 'avg_senti_L6h','fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "                                   'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']]\n",
    "\n",
    "xrp_data_rf_tst2 = xrp_scaled_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h','avg_senti', 'avg_senti_L12h'\n",
    "                                 , 'avg_senti_L2h', 'avg_senti_L6h','fav_percent', 'max_senti', 'min_senti', 'num_tweets',\n",
    "                                   'retweet_percent', 'top_decile_num_tweets', 'top_quartile_num_tweets']]\n",
    "\n",
    "#PCA data\n",
    "xrp_data_rf_tr3 = xrp_trainPCA[['PC1','PC2','PC3','PC4','PC5']].as_matrix()\n",
    "xrp_data_rf_tst3 = xrp_testPCA[['PC1','PC2','PC3','PC4', 'PC5']].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaled raw data\n",
    "xrp_data_rf_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]\n",
    "xrp_data_rf_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h']]\n",
    "\n",
    "#PCA data\n",
    "xrp_data_rf_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1','PC2']]\n",
    "xrp_data_rf_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1','PC2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Evaluation on Test Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "xrp_data_lr_tr3 = xrp_trainPCA[['PC1','PC2', 'PC3','PC4','PC5']].as_matrix()\n",
    "xrp_data_lr_tst3 = xrp_testPCA[['PC1','PC2', 'PC3','PC4','PC5']].as_matrix()\n",
    "model_lr = model_lr = lm.LogisticRegression(C= 0.001, solver='liblinear',class_weight = 'balanced')\n",
    "model_lr.fit(xrp_data_lr_tr3, xrp_target_train)\n",
    "pred_model_lr = model_lr.predict(xrp_data_lr_tst3) \n",
    "pred_probs_model_lr = model_lr.predict_proba(xrp_data_lr_tst3)\n",
    "print ('ROC TR AUC:     LR1 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_lr.predict_proba(xrp_data_lr_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    LR1 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr[:, 1]))\n",
    "\n",
    "\n",
    "#model2\n",
    "xrp_data_lr_tr3 = xrp_trainPCA[['PC2', 'PC3','PC4','PC5']].as_matrix()\n",
    "xrp_data_lr_tst3 = xrp_testPCA[['PC2', 'PC3','PC4','PC5']].as_matrix()\n",
    "model_lr = lm.LogisticRegression(C= 10, solver='liblinear',class_weight = 'balanced')\n",
    "model_lr.fit(xrp_data_lr_tr3, xrp_target_train)\n",
    "pred_model_lr = model_lr.predict(xrp_data_lr_tst3) \n",
    "pred_probs_model_lr = model_lr.predict_proba(xrp_data_lr_tst3)\n",
    "print ('ROC TR AUC:     LR2 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_lr.predict_proba(xrp_data_lr_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    LR2 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr[:, 1]))\n",
    "\n",
    "\n",
    "#model3\n",
    "\n",
    "xrp_data_lr_tst3 = xrp_testPCA[['PC2','PC4','PC5']].as_matrix()\n",
    "model_lr = lm.LogisticRegression(C= 0.1, solver='liblinear',class_weight = 'balanced')\n",
    "model_lr.fit(xrp_data_lr_tr2, xrp_target_train)\n",
    "pred_model_lr = model_lr.predict(xrp_data_lr_tst2) \n",
    "pred_probs_model_lr = model_lr.predict_proba(xrp_data_lr_tst2)\n",
    "print ('ROC TR AUC:     LR3 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_lr.predict_proba(xrp_data_lr_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    LR3 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "xrp_data_lr_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L12h']]\n",
    "xrp_data_lr_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L12h']]\n",
    "model_lr = lm.LogisticRegression(C= 0.001, solver='liblinear',class_weight = 'balanced')\n",
    "model_lr.fit(xrp_data_lr_no_tweet_tr2, xrp_target_train)\n",
    "pred_model_lr = model_lr.predict(xrp_data_lr_no_tweet_tst2) \n",
    "pred_probs_model_lr = model_lr.predict_proba(xrp_data_lr_no_tweet_tst2)\n",
    "print ('ROC TR AUC:     LR1 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_lr.predict_proba(xrp_data_lr_no_tweet_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    LR1 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr[:, 1]))\n",
    "\n",
    "\n",
    "#model2\n",
    "xrp_data_lr_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1','PC2']]\n",
    "xrp_data_lr_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1','PC2']]\n",
    "model_lr = lm.LogisticRegression(C= 0.1, solver='liblinear',class_weight = 'balanced')\n",
    "model_lr.fit(xrp_data_lr_no_tweet_tr3, xrp_target_train)\n",
    "pred_model_lr = model_lr.predict(xrp_data_lr_no_tweet_tst3) \n",
    "pred_probs_model_lr = model_lr.predict_proba(xrp_data_lr_no_tweet_tst3)\n",
    "print ('ROC TR AUC:     LR2 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_lr.predict_proba(xrp_data_lr_no_tweet_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    LR2 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best features for Logistic Regression According to Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp_data_lr_tr3 = xrp_trainPCA[['PC2', 'PC3','PC4','PC5']].as_matrix()\n",
    "xrp_data_lr_tst3 = xrp_testPCA[['PC2', 'PC3','PC4','PC5']].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp_data_lr_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1','PC2']]\n",
    "xrp_data_lr_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1','PC2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr =lm.LogisticRegression(C= 10, solver='liblinear',class_weight = 'balanced')\n",
    "model_lr.fit(xrp_data_lr_tr3, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_no_tweet = lm.LogisticRegression(C= 0.1, solver='liblinear',class_weight = 'balanced')\n",
    "model_lr_no_tweet.fit(xrp_data_lr_no_tweet_tr3, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_lr = model_lr.predict(xrp_data_lr_tst3)\n",
    "#generate class probabilities\n",
    "pred_probs_model_lr = model_lr.predict_proba(xrp_data_lr_tst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  LR - %0.3f'%model_lr.score(xrp_data_lr_tr3, xrp_target_train))\n",
    "print('Test Accuracy:      LR - %0.3f'%model_lr.score(xrp_data_lr_tst3, xrp_target_test))\n",
    "print ('Training ROC AUC:  LR - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_lr.predict_proba(xrp_data_lr_tr3)[:, 1]))\n",
    "print ('Testing ROC AUC:   LR - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_lr_no_tweet = model_lr_no_tweet.predict(xrp_data_lr_no_tweet_tst3)\n",
    "#generate class probabilities\n",
    "pred_probs_model_lr_no_tweet = model_lr_no_tweet.predict_proba(xrp_data_lr_no_tweet_tst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  LR - %0.3f'%model_lr_no_tweet.score(xrp_data_lr_no_tweet_tr3, xrp_target_train))\n",
    "print('Test Accuracy:      LR - %0.3f'%model_lr_no_tweet.score(xrp_data_lr_no_tweet_tst3, xrp_target_test))\n",
    "print ('Training ROC AUC:  LR - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_lr_no_tweet.predict_proba(xrp_data_lr_no_tweet_tr3)[:, 1]))\n",
    "print ('Testing ROC AUC:   LR - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr_no_tweet[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Model Evaluation on Test Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model1\n",
    "model_dt = dt(criterion = 'gini',class_weight = 'balanced', max_depth=5, min_samples_split=0.1, min_samples_leaf=0.1)\n",
    "model_dt.fit(xrp_data_dt_tr2, xrp_target_train)\n",
    "pred_model_dt = model_dt.predict(xrp_data_dt_tst2) \n",
    "pred_probs_model_dt = model_dt.predict_proba(xrp_data_dt_tst2)\n",
    "print ('ROC TR AUC:     DT1 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_dt.predict_proba(xrp_data_dt_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    DT1 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt[:, 1]))\n",
    "\n",
    "#model2\n",
    "model_dt = dt(criterion = 'gini',class_weight = 'balanced', max_depth=5, min_samples_split=0.1, min_samples_leaf=0.1)\n",
    "model_dt.fit(xrp_data_dt_tr3, xrp_target_train)\n",
    "pred_model_dt = model_dt.predict(xrp_data_dt_tst3) \n",
    "pred_probs_model_dt = model_dt.predict_proba(xrp_data_dt_tst3)\n",
    "print ('ROC TR AUC:     DT4 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_dt.predict_proba(xrp_data_dt_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    DT4 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "xrp_data_dt_no_tweet_tr2 = xrp_scaled_no_tweet_tr\n",
    "xrp_data_dt_no_tweet_tst2 = xrp_scaled_no_tweet_tst\n",
    "model_dt = dt(criterion = 'gini',class_weight = 'balanced', max_depth=5, min_samples_split=0.1, min_samples_leaf=0.1)\n",
    "model_dt.fit(xrp_data_dt_no_tweet_tr2, xrp_target_train)\n",
    "pred_model_dt = model_dt.predict(xrp_data_dt_no_tweet_tst2) \n",
    "pred_probs_model_dt = model_dt.predict_proba(xrp_data_dt_no_tweet_tst2)\n",
    "print ('ROC TR AUC:     DT1 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_dt.predict_proba(xrp_data_dt_no_tweet_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    DT1 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt[:, 1]))\n",
    "\n",
    "#model2\n",
    "xrp_data_dt_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L12h','avg_return_L6h']]\n",
    "xrp_data_dt_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L12h','avg_return_L6h']]\n",
    "model_dt = dt(criterion = 'gini',class_weight = 'balanced', max_depth=10, min_samples_split=0.1, min_samples_leaf=0.1)\n",
    "model_dt.fit(xrp_data_dt_no_tweet_tr2, xrp_target_train)\n",
    "pred_model_dt = model_dt.predict(xrp_data_dt_no_tweet_tst2) \n",
    "pred_probs_model_dt = model_dt.predict_proba(xrp_data_dt_no_tweet_tst2)\n",
    "print ('ROC TR AUC:     DT2 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_dt.predict_proba(xrp_data_dt_no_tweet_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    DT2 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt[:, 1]))\n",
    "\n",
    "\n",
    "#model3\n",
    "xrp_data_dt_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1']].as_matrix()\n",
    "xrp_data_dt_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1']].as_matrix()\n",
    "model_dt = dt(criterion = 'gini',class_weight = 'balanced', max_depth=5, min_samples_split=0.2, min_samples_leaf=0.1)\n",
    "model_dt.fit(xrp_data_dt_no_tweet_tr3, xrp_target_train)\n",
    "pred_model_dt = model_dt.predict(xrp_data_dt_no_tweet_tst3) \n",
    "pred_probs_model_dt = model_dt.predict_proba(xrp_data_dt_no_tweet_tst3)\n",
    "print ('ROC TR AUC:     DT3 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_dt.predict_proba(xrp_data_dt_no_tweet_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    DT3 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best features for Decision Tree According to Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp_data_dt_tr2 = xrp_scaled_tr[['avg_return_L6h','avg_return_L12h']]\n",
    "xrp_data_dt_tst2 = xrp_scaled_tst[['avg_return_L6h','avg_return_L12h']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp_data_dt_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L12h','avg_return_L6h']]\n",
    "xrp_data_dt_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L12h','avg_return_L6h']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = dt(criterion = 'gini',class_weight = 'balanced', max_depth=10, min_samples_split=0.1, min_samples_leaf=0.1)\n",
    "model_dt.fit(xrp_data_dt_tr2, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_no_tweet = dt(criterion = 'gini',class_weight = 'balanced', max_depth=10, min_samples_split=0.1, min_samples_leaf=0.1)\n",
    "model_dt_no_tweet.fit(xrp_data_dt_no_tweet_tr2, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_dt = model_dt.predict(xrp_data_dt_tst2) \n",
    "#generate class probabilities\n",
    "pred_probs_model_dt = model_dt.predict_proba(xrp_data_dt_tst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  DT - %0.3f'%model_dt.score(xrp_data_dt_tr2, xrp_target_train))\n",
    "print('Test Accuracy:      DT - %0.3f'%model_dt.score(xrp_data_dt_tst2, xrp_target_test))\n",
    "print ('Training ROC AUC:  DT - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_dt.predict_proba(xrp_data_dt_tr2)[:, 1]))\n",
    "print ('Testing ROC AUC:   DT - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_dt_no_tweet = model_dt_no_tweet.predict(xrp_data_dt_no_tweet_tst2) \n",
    "#generate class probabilities\n",
    "pred_probs_model_dt_no_tweet = model_dt_no_tweet.predict_proba(xrp_data_dt_no_tweet_tst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  DT - %0.3f'%model_dt_no_tweet.score(xrp_data_dt_no_tweet_tr2, xrp_target_train))\n",
    "print('Test Accuracy:      DT - %0.3f'%model_dt_no_tweet.score(xrp_data_dt_no_tweet_tst2, xrp_target_test))\n",
    "print ('Training ROC AUC:  DT - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_dt_no_tweet.predict_proba(xrp_data_dt_no_tweet_tr2)[:, 1]))\n",
    "print ('Testing ROC AUC:   DT - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt_no_tweet[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Model Evaluation on Test Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model1\n",
    "xrp_data_svm_tr2 = xrp_scaled_tr\n",
    "xrp_data_svm_tst2 = xrp_scaled_tst\n",
    "model_svm = svm.SVC(probability = True, kernel = 'rbf', C= 1, class_weight = 'balanced', gamma = 1)\n",
    "model_svm.fit(xrp_data_svm_tr2, xrp_target_train)\n",
    "pred_model_svm = model_svm.predict(xrp_data_svm_tst2) \n",
    "pred_probs_model_svm = model_svm.predict_proba(xrp_data_svm_tst2)\n",
    "print ('ROC TR AUC:     SVM1 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_svm.predict_proba(xrp_data_svm_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    SVM1 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_svm[:, 1]))\n",
    "\n",
    "\n",
    "#model2\n",
    "xrp_data_svm_tr3 = xrp_trainPCA[['PC1','PC2','PC3','PC4','PC5']].as_matrix()\n",
    "xrp_data_svm_tst3 = xrp_testPCA[['PC1','PC2','PC3','PC4','PC5']].as_matrix()\n",
    "model_svm = svm.SVC(probability = True,kernel = 'rbf',C= 10,class_weight = 'balanced', gamma = 1)\n",
    "model_svm.fit(xrp_data_svm_tr3, xrp_target_train)\n",
    "pred_model_svm = model_svm.predict(xrp_data_svm_tst3) \n",
    "pred_probs_model_svm = model_svm.predict_proba(xrp_data_svm_tst3)\n",
    "print ('ROC TR AUC:     SVM2 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_svm.predict_proba(xrp_data_svm_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    SVM2 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_svm[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "xrp_data_lr_no_tweet_tr2 = xrp_scaled_no_tweet_tr\n",
    "xrp_data_lr_no_tweet_tst2 = xrp_scaled_no_tweet_tst\n",
    "model_svm = svm.SVC(probability = True, kernel = 'rbf', C= 10, class_weight = 'balanced', gamma = 1)\n",
    "model_svm.fit(xrp_data_lr_no_tweet_tr2, xrp_target_train)\n",
    "pred_model_svm = model_svm.predict(xrp_data_lr_no_tweet_tst2) \n",
    "pred_probs_model_svm = model_svm.predict_proba(xrp_data_lr_no_tweet_tst2)\n",
    "print ('ROC TR AUC:     SVM1 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_svm.predict_proba(xrp_data_lr_no_tweet_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    SVM1 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_svm[:, 1]))\n",
    "\n",
    "\n",
    "#model2\n",
    "xrp_data_svm_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1','PC2']].as_matrix()\n",
    "xrp_data_svm_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1','PC2']].as_matrix()\n",
    "model_svm = svm.SVC(probability = True,kernel = 'linear',C= 100,class_weight = 'balanced', gamma = 0.001)\n",
    "model_svm.fit(xrp_data_svm_no_tweet_tr3, xrp_target_train)\n",
    "pred_model_svm = model_svm.predict(xrp_data_svm_no_tweet_tst3) \n",
    "pred_probs_model_svm = model_svm.predict_proba(xrp_data_svm_no_tweet_tst3)\n",
    "print ('ROC TR AUC:     SVM2 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_svm.predict_proba(xrp_data_svm_no_tweet_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    SVM2 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_svm[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  SVM Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm =  svm.SVC(probability = True, kernel = 'rbf', C= 1, class_weight = 'balanced', gamma = 1)\n",
    "model_svm.fit(xrp_data_svm_tr2, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm_no_tweet =  svm.SVC(probability = True,kernel = 'linear',C= 100,class_weight = 'balanced', gamma = 0.001)\n",
    "model_svm_no_tweet.fit(xrp_data_svm_no_tweet_tr3, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_svm = model_svm.predict(xrp_data_svm_tst2)\n",
    "#generate class probabilities\n",
    "pred_probs_model_svm = model_svm.predict_proba(xrp_data_svm_tst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  SVM - %0.3f'%model_svm.score(xrp_data_svm_tr2, xrp_target_train))\n",
    "print('Test Accuracy:      SVM - %0.3f'%model_svm.score(xrp_data_svm_tst2, xrp_target_test))\n",
    "print ('Training ROC AUC:  SVM - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_svm.predict_proba(xrp_data_svm_tr2)[:, 1]))\n",
    "print ('Testing ROC AUC:   SVM - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_svm[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_svm_no_tweet = model_svm_no_tweet.predict(xrp_data_svm_no_tweet_tst3)\n",
    "#generate class probabilities\n",
    "pred_probs_model_svm_no_tweet = model_svm_no_tweet.predict_proba(xrp_data_svm_no_tweet_tst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  SVM - %0.3f'%model_svm_no_tweet.score(xrp_data_svm_no_tweet_tr3, xrp_target_train))\n",
    "print('Test Accuracy:      SVM - %0.3f'%model_svm_no_tweet.score(xrp_data_svm_no_tweet_tst3, xrp_target_test))\n",
    "print ('Training ROC AUC:  SVM - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_svm_no_tweet.predict_proba(xrp_data_svm_no_tweet_tr3)[:, 1]))\n",
    "print ('Testing ROC AUC:   SVM - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_svm_no_tweet[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Naive Bayes Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = nb.GaussianNB()\n",
    "model_nb.fit(xrp_data_nb_tr3, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb_no_tweet = nb.GaussianNB()\n",
    "model_nb_no_tweet.fit(xrp_data_nb_no_tweet_tr3, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_nb = model_nb.predict(xrp_data_nb_tst3)\n",
    "#generate class probabilities\n",
    "pred_probs_model_nb = model_nb.predict_proba(xrp_data_nb_tst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  NB - %0.3f'%model_nb.score(xrp_data_nb_tr3, xrp_target_train))\n",
    "print('Test Accuracy:      NB - %0.3f'%model_nb.score(xrp_data_nb_tst3, xrp_target_test))\n",
    "print ('Training ROC AUC:  NB - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_nb.predict_proba(xrp_data_nb_tr3)[:, 1]))\n",
    "print ('Testing ROC AUC:   NB - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_nb[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_nb_no_tweet = model_nb_no_tweet.predict(xrp_data_nb_no_tweet_tst3)\n",
    "#generate class probabilities\n",
    "pred_probs_model_nb_no_tweet = model_nb_no_tweet.predict_proba(xrp_data_nb_no_tweet_tst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  NB - %0.3f'%model_nb_no_tweet.score(xrp_data_nb_no_tweet_tr3, xrp_target_train))\n",
    "print('Test Accuracy:      NB - %0.3f'%model_nb_no_tweet.score(xrp_data_nb_no_tweet_tst3, xrp_target_test))\n",
    "print ('Training ROC AUC:  NB - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_nb_no_tweet.predict_proba(xrp_data_nb_no_tweet_tr3)[:, 1]))\n",
    "print ('Testing ROC AUC:   NB - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_nb_no_tweet[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation on Test Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "xrp_data_rf_tr2 = xrp_scaled_tr[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h', 'avg_senti',\n",
    "                                   'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'fav_percent',\n",
    "                                   'max_senti', 'min_senti', 'num_tweets', 'retweet_percent',\n",
    "                                   'top_quartile_num_tweets']]\n",
    "xrp_data_rf_tst2 = xrp_scaled_tst[['avg_return_L12h', 'avg_return_L2h', 'avg_return_L6h', 'avg_senti',\n",
    "                                   'avg_senti_L12h', 'avg_senti_L2h', 'avg_senti_L6h', 'fav_percent',\n",
    "                                   'max_senti', 'min_senti', 'num_tweets', 'retweet_percent',\n",
    "                                   'top_quartile_num_tweets']]\n",
    "model_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = False, max_depth = 5, \n",
    "             min_samples_leaf = 0.1, min_samples_split = 0.1, n_estimators = 100)\n",
    "model_rf.fit(xrp_data_rf_tr2, xrp_target_train)\n",
    "pred_model_rf = model_rf.predict(xrp_data_rf_tst2) \n",
    "pred_probs_model_rf = model_rf.predict_proba(xrp_data_rf_tst2)\n",
    "print ('ROC TR AUC:     RF1 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_rf.predict_proba(xrp_data_rf_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    RF1 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf[:, 1]))\n",
    "\n",
    "\n",
    "#model2\n",
    "xrp_data_rf_tr2 = xrp_scaled_tr[['avg_return_L12h', 'avg_return_L6h', 'avg_senti','avg_senti_L2h', 'avg_senti_L6h', \n",
    "                                   'max_senti', 'num_tweets']]\n",
    "xrp_data_rf_tst2 = xrp_scaled_tst[['avg_return_L12h', 'avg_return_L6h', 'avg_senti','avg_senti_L2h', 'avg_senti_L6h', \n",
    "                                   'max_senti', 'num_tweets']]\n",
    "model_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = False, max_depth = 5, \n",
    "            min_samples_leaf = 0.1, min_samples_split = 0.1, n_estimators = 500)\n",
    "model_rf.fit(xrp_data_rf_tr2, xrp_target_train)\n",
    "pred_model_rf = model_rf.predict(xrp_data_rf_tst2) \n",
    "pred_probs_model_rf = model_rf.predict_proba(xrp_data_rf_tst2)\n",
    "print ('ROC TR AUC:     RF2 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_rf.predict_proba(xrp_data_rf_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    RF2 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf[:, 1]))\n",
    "\n",
    "\n",
    "\n",
    "#model3\n",
    "xrp_data_rf_tr3 = xrp_trainPCA[['PC3','PC4']].as_matrix()\n",
    "xrp_data_rf_tst3 = xrp_testPCA[['PC3','PC4']].as_matrix()\n",
    "model_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = True, max_depth = 5, \n",
    "            min_samples_leaf = 0.1, min_samples_split = 0.1, n_estimators = 300)\n",
    "model_rf.fit(xrp_data_rf_tr3, xrp_target_train)\n",
    "pred_model_rf = model_rf.predict(xrp_data_rf_tst3) \n",
    "pred_probs_model_rf = model_rf.predict_proba(xrp_data_rf_tst3)\n",
    "print ('ROC TR AUC:     RF3 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_rf.predict_proba(xrp_data_rf_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    RF3 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf[:, 1]))\n",
    "\n",
    "\n",
    "\n",
    "#model4\n",
    "xrp_data_rf_tr3 = xrp_trainPCA[['PC4']].as_matrix()\n",
    "xrp_data_rf_tst3 = xrp_testPCA[['PC4']].as_matrix()\n",
    "model_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = True, max_depth = 5, \n",
    "            min_samples_leaf = 0.1, min_samples_split = 0.1, n_estimators = 500)\n",
    "model_rf.fit(xrp_data_rf_tr3, xrp_target_train)\n",
    "pred_model_rf = model_rf.predict(xrp_data_rf_tst3) \n",
    "pred_probs_model_rf = model_rf.predict_proba(xrp_data_rf_tst3)\n",
    "print ('ROC TR AUC:     RF4 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_rf.predict_proba(xrp_data_rf_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    RF4 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "xrp_data_rf_no_tweet_tr3 = xrp_train_no_tweet_PCA[['PC1']].as_matrix()\n",
    "xrp_data_rf_no_tweet_tst3 = xrp_test_no_tweet_PCA[['PC1']].as_matrix()\n",
    "model_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = False, max_depth = 5, \n",
    "            min_samples_leaf = 0.1, min_samples_split = 0.1, n_estimators = 100)\n",
    "model_rf.fit(xrp_data_rf_no_tweet_tr3, xrp_target_train)\n",
    "pred_model_rf = model_rf.predict(xrp_data_rf_no_tweet_tst3) \n",
    "pred_probs_model_rf = model_rf.predict_proba(xrp_data_rf_no_tweet_tst3)\n",
    "print ('ROC TR AUC:     RF1 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_rf.predict_proba(xrp_data_rf_no_tweet_tr3)[:, 1]))\n",
    "print ('ROC TST AUC:    RF1 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf[:, 1]))\n",
    "\n",
    "\n",
    "#model2\n",
    "xrp_data_rf_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L6h','avg_return_L12h','avg_return_L2h']]\n",
    "xrp_data_rf_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L6h','avg_return_L12h','avg_return_L2h']]\n",
    "model_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = False, max_depth = 5, \n",
    "            min_samples_leaf = 0.1, min_samples_split = 0.3, n_estimators = 300)\n",
    "model_rf.fit(xrp_data_rf_no_tweet_tr2, xrp_target_train)\n",
    "pred_model_rf = model_rf.predict(xrp_data_rf_no_tweet_tst2) \n",
    "pred_probs_model_rf = model_rf.predict_proba(xrp_data_rf_no_tweet_tst2)\n",
    "print ('ROC TR AUC:     RF2 - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_rf.predict_proba(xrp_data_rf_no_tweet_tr2)[:, 1]))\n",
    "print ('ROC TST AUC:    RF2 - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best features for Random Forest According to Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp_data_rf_tr2 = xrp_scaled_tr[['avg_return_L6h','avg_return_L12h','avg_return_L2h']]\n",
    "xrp_data_rf_tst2 = xrp_scaled_tst[['avg_return_L6h','avg_return_L12h','avg_return_L2h']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xrp_data_rf_no_tweet_tr2 = xrp_scaled_no_tweet_tr[['avg_return_L6h','avg_return_L12h','avg_return_L2h']]\n",
    "xrp_data_rf_no_tweet_tst2 = xrp_scaled_no_tweet_tst[['avg_return_L6h','avg_return_L12h','avg_return_L2h']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Random Forest Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = False, max_depth = 5, \n",
    "            min_samples_leaf = 0.1, min_samples_split = 0.3, n_estimators = 300)\n",
    "\n",
    "model_rf.fit(xrp_data_rf_tr2, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf_no_tweet = rf(class_weight = 'balanced' , random_state = 42 ,bootstrap = False, max_depth = 5, \n",
    "            min_samples_leaf = 0.1, min_samples_split = 0.3, n_estimators = 300)\n",
    "\n",
    "model_rf_no_tweet.fit(xrp_data_rf_no_tweet_tr2, xrp_target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Model to Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_rf = model_rf.predict(xrp_data_rf_tst2)\n",
    "#generate class probabilities\n",
    "pred_probs_model_rf = model_rf.predict_proba(xrp_data_rf_tst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  RF - %0.3f'%model_rf.score(xrp_data_rf_tr2, xrp_target_train))\n",
    "print('Test Accuracy:      RF - %0.3f'%model_rf.score(xrp_data_rf_tst2, xrp_target_test))\n",
    "print ('Training ROC AUC:  RF - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_rf.predict_proba(xrp_data_rf_tr2)[:, 1]))\n",
    "print ('Testing ROC AUC:   RF - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Predict target for the test set\n",
    "pred_model_rf_no_tweet = model_rf_no_tweet.predict(xrp_data_rf_no_tweet_tst2)\n",
    "#generate class probabilities\n",
    "pred_probs_model_rf_no_tweet = model_rf_no_tweet.predict_proba(xrp_data_rf_no_tweet_tst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:  RF - %0.3f'%model_rf_no_tweet.score(xrp_data_rf_no_tweet_tr2, xrp_target_train))\n",
    "print('Test Accuracy:      RF - %0.3f'%model_rf_no_tweet.score(xrp_data_rf_no_tweet_tst2, xrp_target_test))\n",
    "print ('Training ROC AUC:  RF - %0.3f'%metrics.roc_auc_score(xrp_target_train, model_rf_no_tweet.predict_proba(xrp_data_rf_no_tweet_tr2)[:, 1]))\n",
    "print ('Testing ROC AUC:   RF - %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf_no_tweet[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With Tweets\n",
    "#generate class probabilities for target ind = 1\n",
    "pred_probs_target_lr = model_lr.predict_proba(xrp_data_lr_tst3)[:, 1]\n",
    "pred_probs_target_dt = model_dt.predict_proba(xrp_data_dt_tst2)[:, 1]\n",
    "pred_probs_target_svm = model_svm.predict_proba(xrp_data_svm_tst2)[:, 1]\n",
    "pred_probs_target_nb = model_nb.predict_proba(xrp_data_nb_tst3)[:, 1]\n",
    "pred_probs_target_rf = model_rf.predict_proba(xrp_data_rf_tst2)[:, 1]\n",
    "\n",
    "#ROC Curve\n",
    "fpr_lr, tpr_lr, thresholds_lr = metrics.roc_curve(xrp_target_test , pred_probs_target_lr)\n",
    "fpr_dt, tpr_dt, thresholds_dt = metrics.roc_curve(xrp_target_test , pred_probs_target_dt)\n",
    "fpr_svm, tpr_svm, thresholds_svm = metrics.roc_curve(xrp_target_test , pred_probs_target_svm)\n",
    "fpr_nb, tpr_nb, thresholds_nb = metrics.roc_curve(xrp_target_test , pred_probs_target_nb)\n",
    "fpr_rf, tpr_rf, thresholds_rf = metrics.roc_curve(xrp_target_test , pred_probs_target_rf)\n",
    "\n",
    "#Without Tweets\n",
    "#generate class probabilities for target ind = 1\n",
    "pred_probs_target_lr_no_tweet = model_lr_no_tweet.predict_proba(xrp_data_lr_no_tweet_tst3)[:, 1]\n",
    "pred_probs_target_dt_no_tweet = model_dt_no_tweet.predict_proba(xrp_data_dt_no_tweet_tst2)[:, 1]\n",
    "pred_probs_target_svm_no_tweet = model_svm_no_tweet.predict_proba(xrp_data_svm_no_tweet_tst3)[:, 1]\n",
    "pred_probs_target_nb_no_tweet = model_nb_no_tweet.predict_proba(xrp_data_nb_no_tweet_tst3)[:, 1]\n",
    "pred_probs_target_rf_no_tweet = model_rf_no_tweet.predict_proba(xrp_data_rf_no_tweet_tst2)[:, 1]\n",
    "\n",
    "#ROC Curve\n",
    "fpr_lr_no_tweet, tpr_lr_no_tweet, thresholds_lr_no_tweet = metrics.roc_curve(xrp_target_test , pred_probs_target_lr_no_tweet)\n",
    "fpr_dt_no_tweet, tpr_dt_no_tweet, thresholds_dt_no_tweet = metrics.roc_curve(xrp_target_test , pred_probs_target_dt_no_tweet)\n",
    "fpr_svm_no_tweet, tpr_svm_no_tweet, thresholds_svm_no_tweet = metrics.roc_curve(xrp_target_test , pred_probs_target_svm_no_tweet)\n",
    "fpr_nb_no_tweet, tpr_nb_no_tweet, thresholds_nb_no_tweet = metrics.roc_curve(xrp_target_test , pred_probs_target_nb_no_tweet)\n",
    "fpr_rf_no_tweet, tpr_rf_no_tweet, thresholds_rf_no_tweet = metrics.roc_curve(xrp_target_test , pred_probs_target_rf_no_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "#ROC Curve\n",
    "#line to show Logistic Regression\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr_lr, tpr_lr, linewidth= 2.5,linestyle='--', color = '#9b59b6', label='Exp3: LR AUC - %0.3f'% metrics.auc(fpr_lr, tpr_lr))\n",
    "#line to show Logistic Regression\n",
    "plt.plot(fpr_lr_no_tweet, tpr_lr_no_tweet, linewidth= 2.5,linestyle='--', color = '#350050', label='Exp4: LR AUC - %0.3f'% metrics.auc(fpr_lr_no_tweet, tpr_lr_no_tweet))\n",
    "#ref line\n",
    "plt.plot(fpr_lr_no_tweet, fpr_lr_no_tweet, linewidth= 1.5,linestyle='--', color = 'white')\n",
    "#plot details\n",
    "plt.legend( loc=\"lower right\", borderaxespad=0. , fontsize=11)\n",
    "plt.xlim(-0.02 , 1.02)\n",
    "plt.ylim(-0.02 , 1.02)\n",
    "plt.title('Ripple ROC Curve', fontsize=14)\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "#line to show Decision Tree\n",
    "plt.plot(fpr_dt, tpr_dt, linewidth= 2.5,linestyle='--', color = '#3498db', label='Exp3: DT AUC - %0.3f'% metrics.auc(fpr_dt, tpr_dt))\n",
    "#line to show Decision Tree\n",
    "plt.plot(fpr_dt_no_tweet, tpr_dt_no_tweet, linewidth= 2.5,linestyle='--', color = '#00195C', label='Exp4: DT AUC - %0.3f'% metrics.auc(fpr_dt_no_tweet, tpr_dt_no_tweet))\n",
    "#ref line\n",
    "plt.plot(fpr_lr_no_tweet, fpr_lr_no_tweet, linewidth= 1.5,linestyle='--', color = 'white')\n",
    "#plot details\n",
    "plt.legend( loc=\"lower right\", borderaxespad=0. , fontsize=12)\n",
    "plt.xlim(-0.02 , 1.02)\n",
    "plt.ylim(-0.02 , 1.02)\n",
    "plt.title('Ripple ROC Curve', fontsize=14)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "#line to show Support Vector Machine\n",
    "plt.plot(fpr_svm, tpr_svm, linewidth= 2.5,linestyle='--', color = '#e74c3c', label='Exp3: SVM AUC - %0.3f'% metrics.auc(fpr_svm, tpr_svm))\n",
    "#line to show Support Vector Machine\n",
    "plt.plot(fpr_svm_no_tweet, tpr_svm_no_tweet, linewidth= 2.5,linestyle='--', color = '#4e0000', label='Exp4: SVM AUC - %0.3f'% metrics.auc(fpr_svm_no_tweet, tpr_svm_no_tweet))\n",
    "#ref line\n",
    "plt.plot(fpr_lr_no_tweet, fpr_lr_no_tweet, linewidth= 1.5,linestyle='--', color = 'white')\n",
    "#plot details\n",
    "plt.legend( loc=\"lower right\", borderaxespad=0. , fontsize=12)\n",
    "plt.xlim(-0.02 , 1.02)\n",
    "plt.ylim(-0.02 , 1.02)\n",
    "plt.title('Ripple ROC Curve', fontsize=14)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "#line to show Naive Bayes\n",
    "plt.plot(fpr_nb, tpr_nb, linewidth= 2.5,linestyle='--', color = '#34495e', label='Exp3: NB AUC - %0.3f'% metrics.auc(fpr_nb, tpr_nb))\n",
    "#line to show Naive Bayes\n",
    "plt.plot(fpr_nb_no_tweet, tpr_nb_no_tweet, linewidth= 2.5,linestyle='--', color = '#cde2f7', label='Exp4: NB AUC - %0.3f'% metrics.auc(fpr_nb_no_tweet, tpr_nb_no_tweet))\n",
    "#ref line\n",
    "plt.plot(fpr_lr_no_tweet, fpr_lr_no_tweet, linewidth= 1.5,linestyle='--', color = 'white')\n",
    "#plot details\n",
    "plt.legend( loc=\"lower right\", borderaxespad=0. , fontsize=12)\n",
    "plt.xlim(-0.02 , 1.02)\n",
    "plt.ylim(-0.02 , 1.02)\n",
    "plt.title('Ripple ROC Curve', fontsize=14)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "#line to show Random Forest\n",
    "plt.plot(fpr_rf, tpr_rf, linewidth= 2.5,linestyle='--', color = '#2ecc71', label='Exp3: RF AUC - %0.3f'% metrics.auc(fpr_rf, tpr_rf))\n",
    "#line to show Random Forest\n",
    "plt.plot(fpr_rf_no_tweet, tpr_rf_no_tweet, linewidth= 2.5,linestyle='--', color = '#00660b', label='Exp4: RF AUC - %0.3f'% metrics.auc(fpr_rf_no_tweet, tpr_rf_no_tweet))\n",
    "#ref line\n",
    "plt.plot(fpr_lr_no_tweet, fpr_lr_no_tweet, linewidth= 1.5,linestyle='--', color = 'white')\n",
    "#plot details\n",
    "plt.legend( loc=\"lower right\", borderaxespad=0. , fontsize=12)\n",
    "plt.xlim(-0.02 , 1.02)\n",
    "plt.ylim(-0.02 , 1.02)\n",
    "plt.title('Ripple ROC Curve Curve', fontsize=14)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_lr, fp_lr, fn_lr, tp_lr = confusion_matrix(xrp_target_test, pred_model_lr).ravel()\n",
    "tn_dt, fp_dt, fn_dt, tp_dt = confusion_matrix(xrp_target_test, pred_model_dt).ravel()\n",
    "tn_svm, fp_svm, fn_svm, tp_svm = confusion_matrix(xrp_target_test, pred_model_svm).ravel()\n",
    "tn_nb, fp_nb, fn_nb, tp_nb = confusion_matrix(xrp_target_test, pred_model_nb).ravel()\n",
    "tn_rf, fp_rf, fn_rf, tp_rf = confusion_matrix(xrp_target_test, pred_model_rf).ravel()\n",
    "print('Prevalence = %0.3f'%(sum(xrp_target_test)/len(xrp_target_test)))\n",
    "print(' ')\n",
    "\n",
    "print('Logistic Regression Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_lr))\n",
    "print('TN: %d'%tn_lr, 'FP: %d'%fp_lr, 'FN: %d'%fn_lr, 'TP: %d'%tp_lr)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_lr))\n",
    "\n",
    "print('Decision Tree Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_dt))\n",
    "print('TN: %d'%tn_dt, 'FP: %d'%fp_dt, 'FN: %d'%fn_dt, 'TP: %d'%tp_dt)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_dt))\n",
    "\n",
    "print('SVM Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_svm[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_svm))\n",
    "print('TN: %d'%tn_svm, 'FP: %d'%fp_svm, 'FN: %d'%fn_svm, 'TP: %d'%tp_svm)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_svm))\n",
    "\n",
    "print('Naive Bayes Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_nb[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_nb))\n",
    "print('TN: %d'%tn_nb, 'FP: %d'%fp_nb, 'FN: %d'%fn_nb, 'TP: %d'%tp_nb)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_nb))\n",
    "\n",
    "print('Random Forest Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_rf))\n",
    "print('TN: %d'%tn_rf, 'FP: %d'%fp_rf, 'FN: %d'%fn_rf, 'TP: %d'%tp_rf)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_lr_no_tweet, fp_lr_no_tweet, fn_lr_no_tweet, tp_lr_no_tweet = confusion_matrix(xrp_target_test, pred_model_lr_no_tweet).ravel()\n",
    "tn_dt_no_tweet, fp_dt_no_tweet, fn_dt_no_tweet, tp_dt_no_tweet = confusion_matrix(xrp_target_test, pred_model_dt_no_tweet).ravel()\n",
    "tn_svm_no_tweet, fp_svm_no_tweet, fn_svm_no_tweet, tp_svm_no_tweet = confusion_matrix(xrp_target_test, pred_model_svm_no_tweet).ravel()\n",
    "tn_nb_no_tweet, fp_nb_no_tweet, fn_nb_no_tweet, tp_nb_no_tweet = confusion_matrix(xrp_target_test, pred_model_nb_no_tweet).ravel()\n",
    "tn_rf_no_tweet, fp_rf_no_tweet, fn_rf_no_tweet, tp_rf_no_tweet = confusion_matrix(xrp_target_test, pred_model_rf_no_tweet).ravel()\n",
    "print('Prevalence = %0.3f'%(sum(xrp_target_test)/len(xrp_target_test)))\n",
    "print(' ')\n",
    "\n",
    "print('Logistic Regression Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_lr_no_tweet[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_lr_no_tweet))\n",
    "print('TN: %d'%tn_lr_no_tweet, 'FP: %d'%fp_lr_no_tweet, 'FN: %d'%fn_lr_no_tweet, 'TP: %d'%tp_lr_no_tweet)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_lr_no_tweet))\n",
    "\n",
    "print('Decision Tree Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_dt_no_tweet[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_dt_no_tweet))\n",
    "print('TN: %d'%tn_dt_no_tweet, 'FP: %d'%fp_dt_no_tweet, 'FN: %d'%fn_dt_no_tweet, 'TP: %d'%tp_dt_no_tweet)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_dt_no_tweet))\n",
    "\n",
    "print('SVM Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_svm_no_tweet[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_svm_no_tweet))\n",
    "print('TN: %d'%tn_svm_no_tweet, 'FP: %d'%fp_svm_no_tweet, 'FN: %d'%fn_svm_no_tweet, 'TP: %d'%tp_svm_no_tweet)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_svm_no_tweet))\n",
    "\n",
    "print('Naive Bayes Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_nb_no_tweet[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_nb_no_tweet))\n",
    "print('TN: %d'%tn_nb_no_tweet, 'FP: %d'%fp_nb_no_tweet, 'FN: %d'%fn_nb_no_tweet, 'TP: %d'%tp_nb_no_tweet)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_nb_no_tweet))\n",
    "\n",
    "print('Random Forest Model')\n",
    "print('ROC AUC: %0.3f'%metrics.roc_auc_score(xrp_target_test, pred_probs_model_rf_no_tweet[:, 1]))\n",
    "print('Accuracy: %0.3f'%metrics.accuracy_score(xrp_target_test, pred_model_rf_no_tweet))\n",
    "print('TN: %d'%tn_rf_no_tweet, 'FP: %d'%fp_rf_no_tweet, 'FN: %d'%fn_rf_no_tweet, 'TP: %d'%tp_rf_no_tweet)\n",
    "print (metrics.classification_report(xrp_target_test, pred_model_rf_no_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
